---
title: 'Meta Knowledge Condensation for Federated Learning'
date: 2023-01-26
permalink: /posts/2023/01/FedMK/
tags:
  - Learning Note
  - DD
---
{% include base_path %}

Learning note for the paper: Meta Knowledge Condensation for Federated Learning.

{% include toc %}

---

# 阅读论文：Meta Knowledge Condensation for Federated Learning

## 一句话：local clients提取meta knowledge；server-side像普通的训练classifier一样，训练global model
## 1. 要解决什么问题
		a. 目前FL需要多轮次的通讯成本，即使有了one-shot FL 但效果大幅下降。因此本文提出meta knowledge representation method 减少通讯次数。
		b. FL中data heterogeneity issue要解决，如何保证clients不biased。
## 2. 用了什么方法
		a. Client：Dynamic weight assignment & knowledge sharing strategy。
			i. 前者根据real data在model上的predict loss施加一个权重：the weight of each sample is inversely proportional to its prediction loss in the current model。也就是说，在更新distilled data时候，会更相信当下表现较好（loss较小）的sample。
			ii. 后者很naive。假设所有clients不是同时开始的，存在时间差。那么，抛弃unconditional initialization（随机初始化），随机选用他人client在前一刻得到的syn sample作为初始化。减少了异质性。
		b. Server-side：聚合clients的meta knowledge，然后像正常的一样训练。但为了缓解多个clients中数据的biases，引入了额外的synthetic training samples（pseudo meta knowledge）。
		具体而言，对uploaded meta knowledge建模统计，然后从中sample即可（真的ok吗？）。就是用uploaded的数据拟合了一个generator。
		c. Iterative：在central结束训练后，广播网络模型参数和meta knowledge。总体计算复杂度为O(N*n)。
		d. 伪代码：
        ![image-20230126194141957](https://tva1.sinaimg.cn/large/008vxvgGgy1hahadrjrm8j30qp0i7n0z.jpg)
## 3. 效果如何
		a. from 74.07% to 92.95% on MNIST with a restricted communication budget (10 rounds)
		b. Results table:
		![image-20230126194205962](https://tva1.sinaimg.cn/large/008vxvgGgy1hahae3zwq3j30hx0b276y.jpg)
## 4. 还存在什么问题&可借鉴之处
		a. increasing the meta knowledge size can not necessarily improve final performance？ 原论文p12有一定的解读。
		b. 写的还是比较清晰的。intro总体框架就是：快速给出FL在de-centralized manner的通病，给出自己的方法，总分说自己方法。最后说自己在几个dataset上的表现，然后在overall一下，分点说自己的贡献。但整个intro感觉重复了三遍，读来有点话痨了。
		我觉得应该加一点FL和DD的点。
